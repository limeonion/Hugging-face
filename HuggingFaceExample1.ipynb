{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c691bf0d-ab00-4c18-8c0f-e8466e1a4ea2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.7 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (0.12.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (4.11.3)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from aiohttp->datasets) (4.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from importlib-metadata->datasets) (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp37-cp37m-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl (266.3 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (61.2.0)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-win_amd64.whl (2.6 MB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.30.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.1.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.6)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\amitk\\.conda\\envs\\datascience\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.4)\n",
      "Installing collected packages: absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.15.0\n",
      "    Uninstalling absl-py-0.15.0:\n",
      "      Successfully uninstalled absl-py-0.15.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.6.0\n",
      "    Uninstalling tensorboard-2.6.0:\n",
      "      Successfully uninstalled tensorboard-2.6.0\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.1.21 gast-0.4.0 google-pasta-0.2.0 h5py-3.8.0 keras-2.11.0 libclang-15.0.6.1 opt-einsum-3.3.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.30.0 termcolor-2.2.0 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece] -q \n",
    "!pip install datasets\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9344f4-1cb2-4649-96e9-18dd23ee8c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993c3c35-4e6b-48d8-b388-372bc8563e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amitk\\.conda\\envs\\DataScience\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tweet_eval (C:/Users/amitk/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 136.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"tweet_eval\", \"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953e9c4-6599-47e3-b2e5-f93a8d4f9f0f",
   "metadata": {},
   "source": [
    "##### The above code will download the dataset named “tweet_eval” with the task/sub-category “emotion”. We can print and check the dataset,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2884eed-be2f-4f53-ac60-b858c715f6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3257\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1421\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 374\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Print and check some details about the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2dad8-f7c4-4341-b1ed-82990c52e7aa",
   "metadata": {},
   "source": [
    "##### Our dataset is the type of DatasetDict and has the train, validation and test split defined. We can directly access the training set of the dataset as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b5bc61-1c73-4090-8342-3bf44eb0239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 3257\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Select the training set from the dataset\n",
    "train_ds = dataset['train']\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9597c1-7a6a-4f7e-9ff3-b346fb164bcf",
   "metadata": {},
   "source": [
    "##### When we print the training dataset, we can observe that it has two features, i.e. text and label. It also has information about the total number of samples in the dataset. We can find more details about the features by,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a156e441-a707-4bc6-8a06-a5f8d76244f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['anger', 'joy', 'optimism', 'sadness'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "# More details about the features\n",
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d81257-aa98-4fae-80d2-4f2b78ef719a",
   "metadata": {},
   "source": [
    "##### Here we can see that the text feature is of the type String and the label feature is ClassLabel. The ClassLabel type shows the total number of classes and their names, i.e. 4 in our example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f5580-7573-478f-bcdb-537926b0ac1f",
   "metadata": {},
   "source": [
    "##### We can easily explore the dataset if it is Pandas Dataframe type. This can be done as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f804e6-bc94-444f-9c60-4d281eb3de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to pandas dataframe\n",
    "import pandas as pd\n",
    "##########################################################\n",
    "dataset.set_format(\"pandas\")\n",
    "train_df = pd.DataFrame(dataset[\"train\"][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115c781-0cbd-44f2-8801-74390201ae47",
   "metadata": {},
   "source": [
    "##### Now we can easily do basic checks like finding null values and the frequency of labels.  Now let's check for null values in the data frame,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b919aef2-1038-428d-a939-4563f9d5f713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values \n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab388b9f-722c-4399-9bc9-027f4dbea5ed",
   "metadata": {},
   "source": [
    "##### Well, we don't have null values. Before we find the frequency of the labels, let's convert the integers to corresponding label names in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "676982c2-1231-407c-91c5-62be58a83bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to convert integer to string \n",
    "def label_int2str(x):\n",
    "    return dataset[\"train\"].features[\"label\"].int2str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6e2e68-74e3-48ae-9274-e908806cb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a label name \n",
    "train_df['label_name'] = train_df[\"label\"].apply(label_int2str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05d7a3-b9bb-4408-9dc9-9ff349005d85",
   "metadata": {},
   "source": [
    "##### Now let's check the distribution of the labels,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29684f45-ffc1-4171-97c4-82bc7bf2d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the distribution of different labels \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3870f9a8-664d-4e46-b6dc-c226c53c8a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdaaa66-d3a8-4b4c-91de-29c36e018892",
   "metadata": {},
   "source": [
    "##### From the above graph, we can see that our dataset is heavily imbalanced. We can further do processing and data analysis from the data frame, but let’s convert it back to the Dataset type suitable for HuggingFace tokenisers and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28ee107f-af7a-4d1a-a737-bd98dec2c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the format\n",
    "dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0203e-f986-4743-9251-ea0c9bf0bab4",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d886b82-e17a-4626-9b89-93122bc63e85",
   "metadata": {},
   "source": [
    "Tokenisation is the most essential preprocessing step in natural language processing. It is converting unstructured text data into a numerical array based on the mapping present in the text vocabulary. Tokenisation is done as follows:\n",
    "\n",
    "- Find a list of all unique words in the dataset\n",
    "- Associate each word/token with a unique number. This is called vocab.\n",
    "- Encode the dataset using the mapping present in the vocab.\n",
    "\n",
    "There are different ways to perform tokenisation. Some of the prominent ones are:\n",
    "\n",
    "- Word-based tokeniser\n",
    "- Character-based tokeniser\n",
    "- Sub-word based tokeniser\n",
    "\n",
    "HuggingFace uses the sub-word based tokeniser to tokenise the datasets by default. Let’s see how to tokenise our dataset using HuggingFace’s AutoTokenizer class.\n",
    "\n",
    "The most important thing to remember while using HuggingFace Library is: \n",
    "\n",
    "Always use the tokenizer and model belonging to the same model checkpoint while fine-tuning models for custom tasks. This will ensure that both model and tokenizer have the same knowledge about the tokens and their encodings.\n",
    "\n",
    "We are using the Distill-Bert model to fine-tune the tweets_eval dataset. More information about the model can be found in the model card (https://huggingface.co/distilbert-base-uncased). We can access the tokeniser and model weights by using the HuggingFace library just by specifying the model name.\n",
    "\n",
    "Now let’s download and import the tokeniser using the AutoTokenizer module,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2491f36b-1c24-470c-905a-f1a9155c1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980edd19-5cad-41a4-b859-c9ca2c359665",
   "metadata": {},
   "source": [
    "##### HuggingFace will automatically download and cache the tokeniser locally. Now let's see how the tokeniser works with an example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8622c2d0-7e5d-4278-abe3-f438facbd5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized output: {'input_ids': [101, 2023, 2003, 2019, 2742, 1997, 19204, 3989, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Tokenized tokens: ['[CLS]', 'this', 'is', 'an', 'example', 'of', 'token', '##ization', '[SEP]']\n",
      "Tokenized text: [CLS] this is an example of tokenization [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example of tokenization\"\n",
    "output = tokenizer(text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(output['input_ids'])\n",
    "print(f\"Tokenized output: {output}\")\n",
    "print(f\"Tokenized tokens: {tokens}\")\n",
    "print(f\"Tokenized text: {tokenizer.convert_tokens_to_string(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e15d2-1645-4ab2-9c85-31fcb65cfbe7",
   "metadata": {},
   "source": [
    "##### Let’s check some important information about the tokeniser like voacb_size, model_max_length etc.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0b25bcc-f585-43da-ae5d-07551c91976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is : 30522\n",
      "Model max length is : 512\n",
      "Model input names are: ['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size is : {tokenizer.vocab_size}\")\n",
    "print(f\"Model max length is : {tokenizer.model_max_length}\")\n",
    "print(f\"Model input names are: {tokenizer.model_input_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d366857-ff2d-42fd-99bc-1ecd1ed639ab",
   "metadata": {},
   "source": [
    "Model max length defines the maximum number of tokens that a single data sample can have, i.e. in the above case, our model DistilBert can accept text sequences of up to 512 tokens long. Model input names are the fields that the model will take as inputs for training and inference purposes.\n",
    "\n",
    "Since we have seen how the tokeniser works, let’s now tokenise the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6b8b438-15d0-48f9-9237-1a45485b8389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\amitk\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-587858c733976373.arrow\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.27ba/s]\n",
      "Loading cached processed dataset at C:\\Users\\amitk\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-441ea9415f78ee06.arrow\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "# Tokenize entire dataset \n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ca6d8-1c69-403c-8a4a-143143f63f73",
   "metadata": {},
   "source": [
    "Great! We can print the dataset and check that extra fields, i.e. input_ids and attention_mask have been added. Now we are ready for the final step, i.e. training the text classifier.\n",
    "\n",
    "### Training the Text Classifier\n",
    "Let’s start by importing the TFAutoModelForSequenceClassification method from the transformers library. This will automatically download and cache the model provided with the checkpoint name. Since we have defined the checkpoint while downloading the tokeniser, we should also use the same to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c4c5ba6-408d-44ff-8031-b0a2db04cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amitk\\.conda\\envs\\DataScience\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "Downloading (…)\"tf_model.h5\";: 100%|████████████████████████████████████████████████| 363M/363M [00:13<00:00, 27.1MB/s]\n",
      "C:\\Users\\amitk\\.conda\\envs\\DataScience\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amitk\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "num_labels = 4\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_ckpt, \n",
    "                                                             num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb39717-75a4-427d-8eec-75405b05c261",
   "metadata": {},
   "source": [
    "We have installed the pre-trained DistilBert Model along with the newly attached model head; thus, this model can now be trained for classification tasks with 4 output labels. Now let’s prepare our data to train it with our model using the Tensorflow deep learning framework.\n",
    "\n",
    "Before we feed the processed token ids into the model, we need to ensure that the dataset is provided as batches and that each data set has an equal length. To do this, HuggingFace gives us a DataCollator method that automatically performs padding to every sample in a batch. The size of every input data sample is equal to the length of the sample with maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f60bd29-a96c-4935-92ac-dba44c0126b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef661fc-83ca-4788-b802-1520c79310f1",
   "metadata": {},
   "source": [
    "##### Now let’s create the TensorFlow datasets from the tokenised dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "324023e6-e9e9-491c-83c9-8cc40274d3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3257\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "037823e5-7619-4c5e-b201-116d6b8abcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf datasets \n",
    "tf_train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\"], \n",
    "    label_cols=[\"label\"], \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "tf_valid_dataset = tokenized_dataset[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\"], \n",
    "    label_cols=[\"label\"], \n",
    "    shuffle=False, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f034f9da-fbf7-4abc-a6a9-e962986eb768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9f6b1b9-80fb-44ed-a707-46c74342dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimiser, SparseCategoricalCrossEntropy loss function\n",
    "# and SparseCategoricalAccuracy as the metric.\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=metrics.SparseCategoricalAccuracy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85d1960d-8ac7-4dd2-94e5-cd2f48b148ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\amitk\\.conda\\envs\\DataScience\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "51/51 [==============================] - 270s 5s/step - loss: 1.1296 - sparse_categorical_accuracy: 0.5177 - val_loss: 0.7826 - val_sparse_categorical_accuracy: 0.7032\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 261s 5s/step - loss: 0.6071 - sparse_categorical_accuracy: 0.7897 - val_loss: 0.6581 - val_sparse_categorical_accuracy: 0.7567\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 276s 5s/step - loss: 0.3533 - sparse_categorical_accuracy: 0.8898 - val_loss: 0.7307 - val_sparse_categorical_accuracy: 0.7460\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 286s 6s/step - loss: 0.2166 - sparse_categorical_accuracy: 0.9343 - val_loss: 0.7102 - val_sparse_categorical_accuracy: 0.7861\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 276s 5s/step - loss: 0.1258 - sparse_categorical_accuracy: 0.9678 - val_loss: 0.8163 - val_sparse_categorical_accuracy: 0.7647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ca880a888>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now finally, let’s fit the model for 5 epochs,\n",
    "\n",
    "# Fit the model\n",
    "model.fit(tf_train_dataset, \n",
    "          validation_data=tf_valid_dataset, \n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3ebe605-ed73-4cf6-875b-e3bd40bfbdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1.8213118314743042,\n",
       " 3.556459903717041,\n",
       " -1.7373497486114502,\n",
       " -0.2650727927684784]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Great! Now we have trained our model for 5 epochs. Now let’s test the model on some sentences.\n",
    "\n",
    "outputs = model.predict(tokenizer(\"I feeling very happy\")[\"input_ids\"])\n",
    "outputs['logits'][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a126722-db7c-4196-9635-e1460bf9b93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# We have fed the model a sentence in the above code, i.e. I am feeling thrilled. We can observe that this sentence belongs to the emotion of joy. Now let’s check the model prediction,\n",
    "\n",
    "# Apply softmax and pick the label with maximum probability\n",
    "import numpy as np\n",
    "label_int = np.argmax(tf.keras.layers.Softmax()(outputs['logits'][0].tolist()))\n",
    "print(label_int.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a94e3f-dcae-4954-94fd-097d31c5f99e",
   "metadata": {},
   "source": [
    "Since the model outputs just the logits, we need to apply softmax activation to convert the values into probabilities. We use softmax and not sigmoid activation because softmax converts logits of multiple classes into the range 0 to 1, therefore suitable for multi-class classification. Now we got the index of maximum probability, and let’s check which type it corresponds to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d845c84d-c3ba-4be3-8c6c-3118984334c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n"
     ]
    }
   ],
   "source": [
    "print(label_int2str(label_int.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb486fe-06b6-4627-bef7-cde0d3ad1c05",
   "metadata": {},
   "source": [
    "Great! Its joy. Our model quickly picked up the sentiment of the sentence. Now we can further improve this model by training for more epochs or preparing better validation sets etc. Then finally, deploy the model in production settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca43382-89ef-4eec-8133-4d68361bb498",
   "metadata": {},
   "source": [
    "# Conclusion on Text Classifier\n",
    "In this session, we have seen how to train a state-of-the-art text classifier by fine-tuning the pre-trained models from HuggingFace Hub. Pre-trained models made publicly available as a part of HuggingFace Hub are a treasure to the machine learning community. As seen above, we can quickly load and train on our datasets instead of training from scratch, which forms the basis for Transfer Learning. Through the number of models and resources available, we can experiment and create new models that might someday revolutionise the world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-DataScience]",
   "language": "python",
   "name": "conda-env-.conda-DataScience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
